{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc127b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb289a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10460, 5523)\n",
      "Validation shape: (2243, 5523)\n",
      "Test shape: (2242, 5523)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('kinase_data_splits.npz')\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b2559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sampling Layer ---\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Reparameterization trick layer.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "# --- Recurrent VAE Model ---\n",
    "class RecurrentVAE(Model):\n",
    "    def __init__(self, sequence_length=263, input_dim=21, latent_dim=32, lstm_units=128, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          sequence_length: Number of amino acids per sequence (e.g., 263)\n",
    "          input_dim: Number of features per amino acid (e.g., 21 for one-hot encoding)\n",
    "          latent_dim: Dimensionality of the latent space (e.g., 32)\n",
    "          lstm_units: Number of units in the LSTM layers (e.g., 128)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "\n",
    "        # ---- Encoder ----\n",
    "        # LSTM encoder: returns the final hidden state only.\n",
    "        self.encoder_lstm = layers.LSTM(lstm_units, return_state=True, return_sequences=False)\n",
    "        self.z_mean_dense = layers.Dense(latent_dim)\n",
    "        self.z_log_var_dense = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "        # ---- Decoder ----\n",
    "        # Repeat latent vector over the time dimension:\n",
    "        self.repeat_vector = layers.RepeatVector(sequence_length)\n",
    "        # LSTM decoder that returns sequences.\n",
    "        self.decoder_lstm = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        # Dense output layer to get back probability distribution over 21 channels.\n",
    "        self.output_dense = layers.TimeDistributed(layers.Dense(input_dim, activation='softmax'))\n",
    "\n",
    "    def encode(self, inputs, training=False):\n",
    "        # inputs shape: (batch, sequence_length, input_dim)\n",
    "        # Run the LSTM; we ignore the cell state.\n",
    "        _, state_h, _ = self.encoder_lstm(inputs)\n",
    "        z_mean = self.z_mean_dense(state_h)\n",
    "        z_log_var = self.z_log_var_dense(state_h)\n",
    "        z = self.sampling([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "    def decode(self, z, training=False):\n",
    "        # Repeat z over time steps to form a sequence.\n",
    "        x = self.repeat_vector(z)  # shape: (batch, sequence_length, latent_dim)\n",
    "        # Decode using LSTM.\n",
    "        x = self.decoder_lstm(x)     # shape: (batch, sequence_length, lstm_units)\n",
    "        reconstruction = self.output_dense(x)  # shape: (batch, sequence_length, input_dim)\n",
    "        return reconstruction\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var, z = self.encode(inputs, training=training)\n",
    "        reconstruction = self.decode(z, training=training)\n",
    "        # Compute KL divergence loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstruction\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RecurrentVAE, self).get_config()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'input_dim': self.input_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'lstm_units': self.lstm_units\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff43956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"recurrent_vae\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"recurrent_vae\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">76,800</span> │\n",
       "│                                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>))              │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sampling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">263</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">263</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">263</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,709</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ((\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m),   │        \u001b[38;5;34m76,800\u001b[0m │\n",
       "│                                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m))              │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sampling (\u001b[38;5;33mSampling\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m263\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m263\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m82,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m263\u001b[0m, \u001b[38;5;34m21\u001b[0m)           │         \u001b[38;5;34m2,709\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170,197</span> (664.83 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m170,197\u001b[0m (664.83 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170,197</span> (664.83 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m170,197\u001b[0m (664.83 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - loss: 3.1134 - val_loss: 3.0404\n",
      "Epoch 2/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 3.0255 - val_loss: 2.9751\n",
      "Epoch 3/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - loss: 2.9687 - val_loss: 2.9596\n",
      "Epoch 4/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 1s/step - loss: 2.9582 - val_loss: 2.9553\n",
      "Epoch 5/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - loss: 2.9550 - val_loss: 2.9533\n",
      "Epoch 6/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - loss: 2.9535 - val_loss: 2.9517\n",
      "Epoch 7/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - loss: 2.9517 - val_loss: 2.9505\n",
      "Epoch 8/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.9503 - val_loss: 2.9496\n",
      "Epoch 9/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.9493 - val_loss: 2.9485\n",
      "Epoch 10/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.9481 - val_loss: 2.9468\n",
      "Epoch 11/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.9472 - val_loss: 2.9449\n",
      "Epoch 12/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - loss: 2.9450 - val_loss: 2.9422\n",
      "Epoch 13/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.9415 - val_loss: 2.9390\n",
      "Epoch 14/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.9382 - val_loss: 2.9364\n",
      "Epoch 15/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.9364 - val_loss: 2.9341\n",
      "Epoch 16/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.9335 - val_loss: 2.9323\n",
      "Epoch 17/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 6s/step - loss: 2.9319 - val_loss: 2.9298\n",
      "Epoch 18/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.9287 - val_loss: 2.9270\n",
      "Epoch 19/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.9263 - val_loss: 2.9233\n",
      "Epoch 20/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.9196 - val_loss: 2.9033\n",
      "Epoch 21/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8998 - val_loss: 2.8884\n",
      "Epoch 22/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.8863 - val_loss: 2.8801\n",
      "Epoch 23/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - loss: 2.8772 - val_loss: 2.8721\n",
      "Epoch 24/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.8715 - val_loss: 2.8676\n",
      "Epoch 25/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1035s\u001b[0m 13s/step - loss: 2.8670 - val_loss: 2.8648\n",
      "Epoch 26/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.8642 - val_loss: 2.8618\n",
      "Epoch 27/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.8609 - val_loss: 2.8581\n",
      "Epoch 28/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - loss: 2.8576 - val_loss: 2.8553\n",
      "Epoch 29/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8532 - val_loss: 2.8496\n",
      "Epoch 30/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.8498 - val_loss: 2.8466\n",
      "Epoch 31/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8467 - val_loss: 2.8443\n",
      "Epoch 32/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8438 - val_loss: 2.8444\n",
      "Epoch 33/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.8418 - val_loss: 2.8399\n",
      "Epoch 34/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.8403 - val_loss: 2.8382\n",
      "Epoch 35/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.8389 - val_loss: 2.8366\n",
      "Epoch 36/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8361 - val_loss: 2.8353\n",
      "Epoch 37/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8342 - val_loss: 2.8338\n",
      "Epoch 38/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8331 - val_loss: 2.8315\n",
      "Epoch 39/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8317 - val_loss: 2.8303\n",
      "Epoch 40/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.8298 - val_loss: 2.8293\n",
      "Epoch 41/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8272 - val_loss: 2.8265\n",
      "Epoch 42/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 6s/step - loss: 2.8257 - val_loss: 2.8243\n",
      "Epoch 43/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.8216 - val_loss: 2.8199\n",
      "Epoch 44/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.8175 - val_loss: 2.8162\n",
      "Epoch 45/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.8135 - val_loss: 2.8115\n",
      "Epoch 46/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8083 - val_loss: 2.8084\n",
      "Epoch 47/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 2.8052 - val_loss: 2.8044\n",
      "Epoch 48/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.8025 - val_loss: 2.8022\n",
      "Epoch 49/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.8000 - val_loss: 2.8010\n",
      "Epoch 50/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7984 - val_loss: 2.7967\n",
      "Epoch 51/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7963 - val_loss: 2.7950\n",
      "Epoch 52/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7925 - val_loss: 2.7929\n",
      "Epoch 53/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 4s/step - loss: 2.7912 - val_loss: 2.7910\n",
      "Epoch 54/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7898 - val_loss: 2.7885\n",
      "Epoch 55/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7867 - val_loss: 2.7866\n",
      "Epoch 56/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7844 - val_loss: 2.7846\n",
      "Epoch 57/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7856 - val_loss: 2.7835\n",
      "Epoch 58/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7816 - val_loss: 2.7814\n",
      "Epoch 59/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.7786 - val_loss: 2.7807\n",
      "Epoch 60/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7786 - val_loss: 2.7779\n",
      "Epoch 61/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7743 - val_loss: 2.7749\n",
      "Epoch 62/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7738 - val_loss: 2.7728\n",
      "Epoch 63/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7713 - val_loss: 2.7717\n",
      "Epoch 64/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7693 - val_loss: 2.7687\n",
      "Epoch 65/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7664 - val_loss: 2.7661\n",
      "Epoch 66/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7644 - val_loss: 2.7633\n",
      "Epoch 67/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7618 - val_loss: 2.7611\n",
      "Epoch 68/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - loss: 2.7586 - val_loss: 2.7583\n",
      "Epoch 69/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 1s/step - loss: 2.7567 - val_loss: 2.7558\n",
      "Epoch 70/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.7551 - val_loss: 2.7528\n",
      "Epoch 71/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7522 - val_loss: 2.7502\n",
      "Epoch 72/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7492 - val_loss: 2.7490\n",
      "Epoch 73/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.7456 - val_loss: 2.7454\n",
      "Epoch 74/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - loss: 2.7431 - val_loss: 2.7432\n",
      "Epoch 75/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7401 - val_loss: 2.7416\n",
      "Epoch 76/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7418 - val_loss: 2.7396\n",
      "Epoch 77/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7374 - val_loss: 2.7368\n",
      "Epoch 78/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7337 - val_loss: 2.7363\n",
      "Epoch 79/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - loss: 2.7341 - val_loss: 2.7361\n",
      "Epoch 80/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7328 - val_loss: 2.7315\n",
      "Epoch 81/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7294 - val_loss: 2.7301\n",
      "Epoch 82/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7287 - val_loss: 2.7300\n",
      "Epoch 83/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7279 - val_loss: 2.7270\n",
      "Epoch 84/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7257 - val_loss: 2.7247\n",
      "Epoch 85/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7224 - val_loss: 2.7225\n",
      "Epoch 86/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7216 - val_loss: 2.7231\n",
      "Epoch 87/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7199 - val_loss: 2.7195\n",
      "Epoch 88/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - loss: 2.7178 - val_loss: 2.7177\n",
      "Epoch 89/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7171 - val_loss: 2.7156\n",
      "Epoch 90/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - loss: 2.7149 - val_loss: 2.7149\n",
      "Epoch 91/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7135 - val_loss: 2.7128\n",
      "Epoch 92/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7130 - val_loss: 2.7114\n",
      "Epoch 93/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7103 - val_loss: 2.7097\n",
      "Epoch 94/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7081 - val_loss: 2.7084\n",
      "Epoch 95/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7069 - val_loss: 2.7061\n",
      "Epoch 96/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - loss: 2.7031 - val_loss: 2.7042\n",
      "Epoch 97/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7025 - val_loss: 2.7039\n",
      "Epoch 98/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.7021 - val_loss: 2.7017\n",
      "Epoch 99/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - loss: 2.6989 - val_loss: 2.6997\n",
      "Epoch 100/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - loss: 2.6986 - val_loss: 2.7009\n"
     ]
    }
   ],
   "source": [
    "# --- Usage Example ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    sequence_length = 263\n",
    "    input_dim = 21\n",
    "    # Reshape flattened data to (batch, sequence_length, input_dim)\n",
    "    X_train_seq = X_train.reshape(-1, sequence_length, input_dim)\n",
    "    X_val_seq = X_val.reshape(-1, sequence_length, input_dim)\n",
    "    \n",
    "    # Instantiate and compile the Recurrent VAE model.\n",
    "    rec_vae = RecurrentVAE(sequence_length=sequence_length, input_dim=input_dim, latent_dim=32, lstm_units=128)\n",
    "    rec_vae.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='categorical_crossentropy')\n",
    "    \n",
    "    # Build the model with a dummy input to initialize layers.\n",
    "    dummy_input = np.zeros((1, sequence_length, input_dim), dtype=np.float32)\n",
    "    _ = rec_vae(dummy_input)\n",
    "    rec_vae.summary()\n",
    "    \n",
    "    # Define callbacks.\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint('best_recurrent_vae.keras', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train the model.\n",
    "    history = rec_vae.fit(\n",
    "        X_train_seq, X_train_seq,\n",
    "        validation_data=(X_val_seq, X_val_seq),\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Save final model in native Keras format.\n",
    "    rec_vae.save('final_recurrent_vae.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4091834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
